{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TigerTail Docs","text":"<p>This site contains the project documentation for the <code>TigerTail</code> library. TigerTail is an event analysis library that helps analysts calculate non-stationary time windows prior to events of interest across multivariate time series data sets. This documentation site aims to explain the use and function of TigerTail classes and methods and to show examples of how to apply them to the analysis of different datasets.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<p>The TigerTail documentation consists of four separate parts:</p> <ol> <li>Tutorials</li> <li>How-To Guides</li> <li>Reference</li> <li>Explanation</li> </ol> <p>Quickly find what you're looking for depending on your use case by looking at the different pages.</p>"},{"location":"birthday-gift-example/","title":"Birthday Gift Example","text":"<pre><code>import sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsys.path.insert(0, '../TT')\nfrom tigertail import TimeFrame, TimeSeries, EventSeries\n</code></pre> <p>Here we generate a dataframe with the timestamps at which Cassie receives a gift during her birthday and her rating of each gift. We choose the gift timestamps at random from a date_range of timestamps from 11/18/1999 00:00:00 to 11/19/1999 00:00:00.</p> <pre><code>timestamp = pd.date_range('1999-11-18 00:00:00', '1999-11-19 00:00:00', freq='1min')\n\ngift_timestamp = np.random.choice(timestamp, size=100, replace=False)\ngift_timestamp\n</code></pre> <pre>\n<code>array(['1999-11-18T03:39:00.000000000', '1999-11-18T06:32:00.000000000',\n       '1999-11-18T03:29:00.000000000', '1999-11-18T05:18:00.000000000',\n       '1999-11-18T15:37:00.000000000', '1999-11-18T22:54:00.000000000',\n       '1999-11-18T11:29:00.000000000', '1999-11-18T11:01:00.000000000',\n       '1999-11-18T20:29:00.000000000', '1999-11-18T05:40:00.000000000',\n       '1999-11-18T09:12:00.000000000', '1999-11-18T12:10:00.000000000',\n       '1999-11-18T13:26:00.000000000', '1999-11-18T08:03:00.000000000',\n       '1999-11-18T07:33:00.000000000', '1999-11-18T09:42:00.000000000',\n       '1999-11-18T14:00:00.000000000', '1999-11-18T12:00:00.000000000',\n       '1999-11-18T23:24:00.000000000', '1999-11-18T16:43:00.000000000',\n       '1999-11-18T22:02:00.000000000', '1999-11-18T06:46:00.000000000',\n       '1999-11-18T11:20:00.000000000', '1999-11-18T21:18:00.000000000',\n       '1999-11-18T04:57:00.000000000', '1999-11-18T09:10:00.000000000',\n       '1999-11-18T05:31:00.000000000', '1999-11-18T21:11:00.000000000',\n       '1999-11-18T12:44:00.000000000', '1999-11-18T01:14:00.000000000',\n       '1999-11-18T11:34:00.000000000', '1999-11-18T20:03:00.000000000',\n       '1999-11-18T01:35:00.000000000', '1999-11-18T16:49:00.000000000',\n       '1999-11-18T16:08:00.000000000', '1999-11-18T17:11:00.000000000',\n       '1999-11-18T12:27:00.000000000', '1999-11-18T09:50:00.000000000',\n       '1999-11-18T14:15:00.000000000', '1999-11-18T08:57:00.000000000',\n       '1999-11-18T22:24:00.000000000', '1999-11-18T04:10:00.000000000',\n       '1999-11-18T23:58:00.000000000', '1999-11-18T13:22:00.000000000',\n       '1999-11-18T10:09:00.000000000', '1999-11-18T04:20:00.000000000',\n       '1999-11-18T03:11:00.000000000', '1999-11-18T07:47:00.000000000',\n       '1999-11-18T14:34:00.000000000', '1999-11-18T17:47:00.000000000',\n       '1999-11-18T20:38:00.000000000', '1999-11-18T04:49:00.000000000',\n       '1999-11-18T11:53:00.000000000', '1999-11-18T02:00:00.000000000',\n       '1999-11-18T02:53:00.000000000', '1999-11-18T23:49:00.000000000',\n       '1999-11-18T01:04:00.000000000', '1999-11-18T01:31:00.000000000',\n       '1999-11-18T00:23:00.000000000', '1999-11-18T11:43:00.000000000',\n       '1999-11-18T16:21:00.000000000', '1999-11-18T21:44:00.000000000',\n       '1999-11-18T19:31:00.000000000', '1999-11-18T09:48:00.000000000',\n       '1999-11-18T18:08:00.000000000', '1999-11-18T16:37:00.000000000',\n       '1999-11-18T06:44:00.000000000', '1999-11-18T03:09:00.000000000',\n       '1999-11-18T23:40:00.000000000', '1999-11-18T14:42:00.000000000',\n       '1999-11-18T18:33:00.000000000', '1999-11-18T05:37:00.000000000',\n       '1999-11-18T10:34:00.000000000', '1999-11-18T01:00:00.000000000',\n       '1999-11-18T23:03:00.000000000', '1999-11-18T17:40:00.000000000',\n       '1999-11-18T02:11:00.000000000', '1999-11-18T13:39:00.000000000',\n       '1999-11-18T05:54:00.000000000', '1999-11-18T02:43:00.000000000',\n       '1999-11-18T17:43:00.000000000', '1999-11-18T04:43:00.000000000',\n       '1999-11-18T04:14:00.000000000', '1999-11-18T15:23:00.000000000',\n       '1999-11-18T06:06:00.000000000', '1999-11-18T17:59:00.000000000',\n       '1999-11-18T18:23:00.000000000', '1999-11-18T12:29:00.000000000',\n       '1999-11-18T00:11:00.000000000', '1999-11-18T07:04:00.000000000',\n       '1999-11-18T05:55:00.000000000', '1999-11-18T10:00:00.000000000',\n       '1999-11-18T16:00:00.000000000', '1999-11-18T08:32:00.000000000',\n       '1999-11-18T03:35:00.000000000', '1999-11-18T08:48:00.000000000',\n       '1999-11-18T01:28:00.000000000', '1999-11-18T11:18:00.000000000',\n       '1999-11-18T09:19:00.000000000', '1999-11-18T03:00:00.000000000'],\n      dtype='datetime64[ns]')</code>\n</pre> <pre><code>gift_ratings = np.random.random(100)\ngift_df = pd.DataFrame({'gift_rating': gift_ratings}, index=gift_timestamp)\ngift_df.sort_index(inplace=True)\nprint(gift_df)\ngift_df.plot(title='Gifts and their ratings', figsize=(20,10), marker='*')\n</code></pre> <pre>\n<code>                     gift_rating\n1999-11-18 00:11:00     0.676726\n1999-11-18 00:23:00     0.452966\n1999-11-18 01:00:00     0.384324\n1999-11-18 01:04:00     0.524974\n1999-11-18 01:14:00     0.618376\n...                          ...\n1999-11-18 23:03:00     0.164931\n1999-11-18 23:24:00     0.457748\n1999-11-18 23:40:00     0.726104\n1999-11-18 23:49:00     0.115720\n1999-11-18 23:58:00     0.044063\n\n[100 rows x 1 columns]\n</code>\n</pre> <pre>\n<code>&lt;Axes: title={'center': 'Gifts and their ratings'}&gt;</code>\n</pre> <p>Now we create an EventSeries object using the birthday gift dataframe. We specify <code>np.mean</code> as the default aggregation function.</p> <pre><code>gift_es = EventSeries(gift_df, agg_func=np.mean)\n</code></pre> <p>We demonstrate how to window the gift event series data with 1 hour and 5 minute windows.</p> <pre><code># demo the EventSeries window function\n# see here for list of accepted freq aliases: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\n\nwindow_es_h = gift_es.window('1h')\nwindow_es_h\n</code></pre> <pre>\n<code>1999-11-18 00:00:00    0.564846\n1999-11-18 01:00:00    0.519829\n1999-11-18 02:00:00    0.451173\n1999-11-18 03:00:00    0.713648\n1999-11-18 04:00:00    0.590792\n1999-11-18 05:00:00    0.468267\n1999-11-18 06:00:00    0.216276\n1999-11-18 07:00:00    0.574016\n1999-11-18 08:00:00    0.525572\n1999-11-18 09:00:00    0.465826\n1999-11-18 10:00:00    0.272272\n1999-11-18 11:00:00    0.310394\n1999-11-18 12:00:00    0.642419\n1999-11-18 13:00:00    0.391932\n1999-11-18 14:00:00    0.646176\n1999-11-18 15:00:00    0.630332\n1999-11-18 16:00:00    0.456948\n1999-11-18 17:00:00    0.700433\n1999-11-18 18:00:00    0.663660\n1999-11-18 19:00:00    0.005790\n1999-11-18 20:00:00    0.314438\n1999-11-18 21:00:00    0.363702\n1999-11-18 22:00:00    0.625112\n1999-11-18 23:00:00    0.301713\nFreq: h, dtype: float64</code>\n</pre> <pre><code>window_es_h.plot(title='Average gift rating for every 1 hr window', marker='*')\n</code></pre> <pre>\n<code>&lt;Axes: title={'center': 'Average gift rating for every 1 hr window'}&gt;</code>\n</pre> <pre><code>window_es_min = gift_es.window('5min')\nwindow_es_min\n</code></pre> <pre>\n<code>1999-11-18 00:10:00    0.676726\n1999-11-18 00:15:00    0.000000\n1999-11-18 00:20:00    0.452966\n1999-11-18 00:25:00    0.000000\n1999-11-18 00:30:00    0.000000\n                         ...   \n1999-11-18 23:35:00    0.000000\n1999-11-18 23:40:00    0.726104\n1999-11-18 23:45:00    0.115720\n1999-11-18 23:50:00    0.000000\n1999-11-18 23:55:00    0.044063\nFreq: 5min, Length: 286, dtype: float64</code>\n</pre> <pre><code>window_es_min.plot(title='Average gift rating for every 5 min window', marker='*', figsize=(20,10))\n</code></pre> <pre>\n<code>&lt;Axes: title={'center': 'Average gift rating for every 5 min window'}&gt;</code>\n</pre> <p>Now we generate the heart rate per minute dataset. First, we generate random heart rates from 60 to 130 for each minute of the day:</p> <pre><code>hr_values = np.random.randint(60, 130, size=1441)\nhr_values\nhr_df = pd.DataFrame({'heart_rate': hr_values}, index=timestamp)\nhr_df.sort_index(inplace=True)\nhr_df.plot(title='Heart rate per minute', linestyle='-', figsize=[25,10])\n</code></pre> <pre>\n<code>&lt;Axes: title={'center': 'Heart rate per minute'}&gt;</code>\n</pre> <p>The code in the following cell adds to the random heart rates generated in the previous cell so that they make more sense in our context, i.e. heart rate increases when the time gets closer to a gift being given.</p> <pre><code>avg_hr = (np.sum(hr_values) / hr_values.size).item()\n\nnew_hr_values = []\nstart = 0\nfor time in gift_df.index:\n    t_e = (60 * time.hour) + time.minute\n    for t in range(start, t_e+1):\n        if t == t_e:\n            # add a peak heart rate value of 160 to new_hr_values\n            high_hr = np.random.randint(156, 166)\n            new_hr_values.append(high_hr)\n            start = t + 1\n            continue\n        else:\n            # add 1/t_i-t_e to new_hr_values\n            new_hr = round(avg_hr + 60 * (1 / (t_e - t)))\n            new_hr_values.append(new_hr)\n\nif t_e+1 &amp;lt;= timestamp.size:\n    for i in range(t_e+1, timestamp.size):\n        new_hr_values.append(avg_hr)\n\nnew_hr_values\n</code></pre> <pre>\n<code>[100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 165,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 159,\n 96,\n 96,\n 96,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 158,\n 115,\n 125,\n 155,\n 156,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 157,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 165,\n 125,\n 155,\n 165,\n 115,\n 125,\n 155,\n 161,\n 97,\n 97,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 162,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 165,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 162,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 161,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 164,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 165,\n 155,\n 157,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 157,\n 107,\n 110,\n 115,\n 125,\n 155,\n 162,\n 115,\n 125,\n 155,\n 165,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 165,\n 115,\n 125,\n 155,\n 156,\n 107,\n 110,\n 115,\n 125,\n 155,\n 163,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 160,\n 107,\n 110,\n 115,\n 125,\n 155,\n 159,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 160,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 161,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 165,\n 107,\n 110,\n 115,\n 125,\n 155,\n 164,\n 125,\n 155,\n 163,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 163,\n 161,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 160,\n 97,\n 97,\n 97,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 159,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 159,\n 155,\n 156,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 160,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 163,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 162,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 160,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 157,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 157,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 160,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 162,\n 155,\n 162,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 162,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 157,\n 107,\n 110,\n 115,\n 125,\n 155,\n 163,\n 155,\n 161,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 159,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 156,\n 97,\n 97,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 162,\n 97,\n 97,\n 97,\n 97,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 157,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 158,\n 155,\n 158,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 160,\n 110,\n 115,\n 125,\n 155,\n 163,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 163,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 157,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 163,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 162,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 165,\n 155,\n 163,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 161,\n 96,\n 96,\n 96,\n 96,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 161,\n 115,\n 125,\n 155,\n 162,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 157,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 165,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 164,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 157,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 157,\n 96,\n 96,\n 96,\n 96,\n 96,\n 96,\n 96,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 161,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 161,\n 97,\n 98,\n 98,\n 98,\n 98,\n 98,\n 98,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 162,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 163,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 163,\n 99,\n 99,\n 99,\n 100,\n 100,\n 101,\n 101,\n 102,\n 103,\n 105,\n 107,\n 110,\n 115,\n 125,\n 155,\n 156,\n 107,\n 110,\n ...]</code>\n</pre> <pre><code>new_hr_df = pd.DataFrame({'heart_rate': new_hr_values}, index=timestamp)\nnew_hr_df.plot(title='New heart rate per minute values', marker='*', figsize=(25,10))\n</code></pre> <pre>\n<code>&lt;Axes: title={'center': 'New heart rate per minute values'}&gt;</code>\n</pre> <p>We create a TimeSeries object with the heart rate dataframe and also specify <code>np.mean</code> as the aggregation function:</p> <pre><code>hr_ts = TimeSeries(new_hr_df, agg_func=np.mean)\n</code></pre> <pre><code># demo TimeSeries window function\n# see here for list of accepted freq aliases: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\n\nhr_ts.window('1h').plot(title='Average heart rate (bpm) over each hour', marker='*')\n</code></pre> <pre>\n<code>&lt;Axes: title={'center': 'Average heart rate (bpm) over each hour'}&gt;</code>\n</pre> <pre><code>hr_ts.window('5min').plot(title='Average heart rate (bpm) over each 5 min', marker='*')\n</code></pre> <pre>\n<code>&lt;Axes: title={'center': 'Average heart rate (bpm) over each 5 min'}&gt;</code>\n</pre> <p>We then create a TimeFrame object, <code>tf</code>, and add the <code>hr_ts</code> and <code>gift_es</code> objects to it.</p> <pre><code>tf = TimeFrame()\ntf['gift_es'] = gift_es\ntf['hr_ts'] = hr_ts\n</code></pre> <pre><code># there's an NaN value for the last timestamp for gift_rating since there are no events \n# at that hour, the last row of the resulting dataframe could be deleted or kept\n\ntf.window('1h')\n</code></pre> gift_rating heart_rate 1999-11-18 00:00:00 0.564846 107.216667 1999-11-18 01:00:00 0.519829 116.300000 1999-11-18 02:00:00 0.451173 111.550000 1999-11-18 03:00:00 0.713648 113.816667 1999-11-18 04:00:00 0.590792 117.133333 1999-11-18 05:00:00 0.468267 115.633333 1999-11-18 06:00:00 0.216276 110.650000 1999-11-18 07:00:00 0.574016 108.783333 1999-11-18 08:00:00 0.525572 111.000000 1999-11-18 09:00:00 0.465826 117.650000 1999-11-18 10:00:00 0.272272 107.300000 1999-11-18 11:00:00 0.310394 120.566667 1999-11-18 12:00:00 0.642419 111.400000 1999-11-18 13:00:00 0.391932 110.266667 1999-11-18 14:00:00 0.646176 109.033333 1999-11-18 15:00:00 0.630332 107.566667 1999-11-18 16:00:00 0.456948 115.683333 1999-11-18 17:00:00 0.700433 113.516667 1999-11-18 18:00:00 0.663660 107.433333 1999-11-18 19:00:00 0.005790 101.983333 1999-11-18 20:00:00 0.314438 107.416667 1999-11-18 21:00:00 0.363702 109.333333 1999-11-18 22:00:00 0.625112 108.000000 1999-11-18 23:00:00 0.301713 114.494980 1999-11-19 00:00:00 0.000000 94.698820 <p>Then we can make a plot of the aggregated gift events and heart rates windowed according to the frequencies that we specify.</p> <pre><code>fig, ax = plt.subplots()\ntf.window('1h').plot(ax=ax, title='Average gift ratings and heart rates over 1 hour windows', marker='*')\nax.legend(['gift rating', 'heart rate'])\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x117d10350&gt;</code>\n</pre> <pre><code>fig, ax = plt.subplots()\ntf.window('5min').plot(figsize=(25,10), ax=ax, title='Average gift ratings and heart rates over 5 min windows', marker='*')\nax.legend(['gift rating', 'heart rate'])\n</code></pre> <pre>\n<code>&lt;matplotlib.legend.Legend at 0x1178bfc90&gt;</code>\n</pre> <p>Now let's try windowing the heart rate data using the gift event data and the <code>nonstationary_window</code> function. First we have to calculate the start and end times of the intervals between events.</p> <pre><code>gift_es.calc_start_end()\ngift_es.ns_window\n</code></pre> <pre>\n<code>[(Timestamp('1999-11-18 00:11:00'), Timestamp('1999-11-18 00:23:00')),\n (Timestamp('1999-11-18 00:23:00'), Timestamp('1999-11-18 01:00:00')),\n (Timestamp('1999-11-18 01:00:00'), Timestamp('1999-11-18 01:04:00')),\n (Timestamp('1999-11-18 01:04:00'), Timestamp('1999-11-18 01:14:00')),\n (Timestamp('1999-11-18 01:14:00'), Timestamp('1999-11-18 01:28:00')),\n (Timestamp('1999-11-18 01:28:00'), Timestamp('1999-11-18 01:31:00')),\n (Timestamp('1999-11-18 01:31:00'), Timestamp('1999-11-18 01:35:00')),\n (Timestamp('1999-11-18 01:35:00'), Timestamp('1999-11-18 02:00:00')),\n (Timestamp('1999-11-18 02:00:00'), Timestamp('1999-11-18 02:11:00')),\n (Timestamp('1999-11-18 02:11:00'), Timestamp('1999-11-18 02:43:00')),\n (Timestamp('1999-11-18 02:43:00'), Timestamp('1999-11-18 02:53:00')),\n (Timestamp('1999-11-18 02:53:00'), Timestamp('1999-11-18 03:00:00')),\n (Timestamp('1999-11-18 03:00:00'), Timestamp('1999-11-18 03:09:00')),\n (Timestamp('1999-11-18 03:09:00'), Timestamp('1999-11-18 03:11:00')),\n (Timestamp('1999-11-18 03:11:00'), Timestamp('1999-11-18 03:29:00')),\n (Timestamp('1999-11-18 03:29:00'), Timestamp('1999-11-18 03:35:00')),\n (Timestamp('1999-11-18 03:35:00'), Timestamp('1999-11-18 03:39:00')),\n (Timestamp('1999-11-18 03:39:00'), Timestamp('1999-11-18 04:10:00')),\n (Timestamp('1999-11-18 04:10:00'), Timestamp('1999-11-18 04:14:00')),\n (Timestamp('1999-11-18 04:14:00'), Timestamp('1999-11-18 04:20:00')),\n (Timestamp('1999-11-18 04:20:00'), Timestamp('1999-11-18 04:43:00')),\n (Timestamp('1999-11-18 04:43:00'), Timestamp('1999-11-18 04:49:00')),\n (Timestamp('1999-11-18 04:49:00'), Timestamp('1999-11-18 04:57:00')),\n (Timestamp('1999-11-18 04:57:00'), Timestamp('1999-11-18 05:18:00')),\n (Timestamp('1999-11-18 05:18:00'), Timestamp('1999-11-18 05:31:00')),\n (Timestamp('1999-11-18 05:31:00'), Timestamp('1999-11-18 05:37:00')),\n (Timestamp('1999-11-18 05:37:00'), Timestamp('1999-11-18 05:40:00')),\n (Timestamp('1999-11-18 05:40:00'), Timestamp('1999-11-18 05:54:00')),\n (Timestamp('1999-11-18 05:54:00'), Timestamp('1999-11-18 05:55:00')),\n (Timestamp('1999-11-18 05:55:00'), Timestamp('1999-11-18 06:06:00')),\n (Timestamp('1999-11-18 06:06:00'), Timestamp('1999-11-18 06:32:00')),\n (Timestamp('1999-11-18 06:32:00'), Timestamp('1999-11-18 06:44:00')),\n (Timestamp('1999-11-18 06:44:00'), Timestamp('1999-11-18 06:46:00')),\n (Timestamp('1999-11-18 06:46:00'), Timestamp('1999-11-18 07:04:00')),\n (Timestamp('1999-11-18 07:04:00'), Timestamp('1999-11-18 07:33:00')),\n (Timestamp('1999-11-18 07:33:00'), Timestamp('1999-11-18 07:47:00')),\n (Timestamp('1999-11-18 07:47:00'), Timestamp('1999-11-18 08:03:00')),\n (Timestamp('1999-11-18 08:03:00'), Timestamp('1999-11-18 08:32:00')),\n (Timestamp('1999-11-18 08:32:00'), Timestamp('1999-11-18 08:48:00')),\n (Timestamp('1999-11-18 08:48:00'), Timestamp('1999-11-18 08:57:00')),\n (Timestamp('1999-11-18 08:57:00'), Timestamp('1999-11-18 09:10:00')),\n (Timestamp('1999-11-18 09:10:00'), Timestamp('1999-11-18 09:12:00')),\n (Timestamp('1999-11-18 09:12:00'), Timestamp('1999-11-18 09:19:00')),\n (Timestamp('1999-11-18 09:19:00'), Timestamp('1999-11-18 09:42:00')),\n (Timestamp('1999-11-18 09:42:00'), Timestamp('1999-11-18 09:48:00')),\n (Timestamp('1999-11-18 09:48:00'), Timestamp('1999-11-18 09:50:00')),\n (Timestamp('1999-11-18 09:50:00'), Timestamp('1999-11-18 10:00:00')),\n (Timestamp('1999-11-18 10:00:00'), Timestamp('1999-11-18 10:09:00')),\n (Timestamp('1999-11-18 10:09:00'), Timestamp('1999-11-18 10:34:00')),\n (Timestamp('1999-11-18 10:34:00'), Timestamp('1999-11-18 11:01:00')),\n (Timestamp('1999-11-18 11:01:00'), Timestamp('1999-11-18 11:18:00')),\n (Timestamp('1999-11-18 11:18:00'), Timestamp('1999-11-18 11:20:00')),\n (Timestamp('1999-11-18 11:20:00'), Timestamp('1999-11-18 11:29:00')),\n (Timestamp('1999-11-18 11:29:00'), Timestamp('1999-11-18 11:34:00')),\n (Timestamp('1999-11-18 11:34:00'), Timestamp('1999-11-18 11:43:00')),\n (Timestamp('1999-11-18 11:43:00'), Timestamp('1999-11-18 11:53:00')),\n (Timestamp('1999-11-18 11:53:00'), Timestamp('1999-11-18 12:00:00')),\n (Timestamp('1999-11-18 12:00:00'), Timestamp('1999-11-18 12:10:00')),\n (Timestamp('1999-11-18 12:10:00'), Timestamp('1999-11-18 12:27:00')),\n (Timestamp('1999-11-18 12:27:00'), Timestamp('1999-11-18 12:29:00')),\n (Timestamp('1999-11-18 12:29:00'), Timestamp('1999-11-18 12:44:00')),\n (Timestamp('1999-11-18 12:44:00'), Timestamp('1999-11-18 13:22:00')),\n (Timestamp('1999-11-18 13:22:00'), Timestamp('1999-11-18 13:26:00')),\n (Timestamp('1999-11-18 13:26:00'), Timestamp('1999-11-18 13:39:00')),\n (Timestamp('1999-11-18 13:39:00'), Timestamp('1999-11-18 14:00:00')),\n (Timestamp('1999-11-18 14:00:00'), Timestamp('1999-11-18 14:15:00')),\n (Timestamp('1999-11-18 14:15:00'), Timestamp('1999-11-18 14:34:00')),\n (Timestamp('1999-11-18 14:34:00'), Timestamp('1999-11-18 14:42:00')),\n (Timestamp('1999-11-18 14:42:00'), Timestamp('1999-11-18 15:23:00')),\n (Timestamp('1999-11-18 15:23:00'), Timestamp('1999-11-18 15:37:00')),\n (Timestamp('1999-11-18 15:37:00'), Timestamp('1999-11-18 16:00:00')),\n (Timestamp('1999-11-18 16:00:00'), Timestamp('1999-11-18 16:08:00')),\n (Timestamp('1999-11-18 16:08:00'), Timestamp('1999-11-18 16:21:00')),\n (Timestamp('1999-11-18 16:21:00'), Timestamp('1999-11-18 16:37:00')),\n (Timestamp('1999-11-18 16:37:00'), Timestamp('1999-11-18 16:43:00')),\n (Timestamp('1999-11-18 16:43:00'), Timestamp('1999-11-18 16:49:00')),\n (Timestamp('1999-11-18 16:49:00'), Timestamp('1999-11-18 17:11:00')),\n (Timestamp('1999-11-18 17:11:00'), Timestamp('1999-11-18 17:40:00')),\n (Timestamp('1999-11-18 17:40:00'), Timestamp('1999-11-18 17:43:00')),\n (Timestamp('1999-11-18 17:43:00'), Timestamp('1999-11-18 17:47:00')),\n (Timestamp('1999-11-18 17:47:00'), Timestamp('1999-11-18 17:59:00')),\n (Timestamp('1999-11-18 17:59:00'), Timestamp('1999-11-18 18:08:00')),\n (Timestamp('1999-11-18 18:08:00'), Timestamp('1999-11-18 18:23:00')),\n (Timestamp('1999-11-18 18:23:00'), Timestamp('1999-11-18 18:33:00')),\n (Timestamp('1999-11-18 18:33:00'), Timestamp('1999-11-18 19:31:00')),\n (Timestamp('1999-11-18 19:31:00'), Timestamp('1999-11-18 20:03:00')),\n (Timestamp('1999-11-18 20:03:00'), Timestamp('1999-11-18 20:29:00')),\n (Timestamp('1999-11-18 20:29:00'), Timestamp('1999-11-18 20:38:00')),\n (Timestamp('1999-11-18 20:38:00'), Timestamp('1999-11-18 21:11:00')),\n (Timestamp('1999-11-18 21:11:00'), Timestamp('1999-11-18 21:18:00')),\n (Timestamp('1999-11-18 21:18:00'), Timestamp('1999-11-18 21:44:00')),\n (Timestamp('1999-11-18 21:44:00'), Timestamp('1999-11-18 22:02:00')),\n (Timestamp('1999-11-18 22:02:00'), Timestamp('1999-11-18 22:24:00')),\n (Timestamp('1999-11-18 22:24:00'), Timestamp('1999-11-18 22:54:00')),\n (Timestamp('1999-11-18 22:54:00'), Timestamp('1999-11-18 23:03:00')),\n (Timestamp('1999-11-18 23:03:00'), Timestamp('1999-11-18 23:24:00')),\n (Timestamp('1999-11-18 23:24:00'), Timestamp('1999-11-18 23:40:00')),\n (Timestamp('1999-11-18 23:40:00'), Timestamp('1999-11-18 23:49:00')),\n (Timestamp('1999-11-18 23:49:00'), Timestamp('1999-11-18 23:58:00'))]</code>\n</pre> <p>Now we window the heart rate time series data using the timestamps of the gift event series data:</p> <pre><code>ns_window_df = hr_ts.nonstationary_window(gift_es)\nns_window_df\n</code></pre> heart_rate 1999-11-18 00:11:00 115.750000 1999-11-18 00:23:00 115.250000 1999-11-18 01:00:00 103.216216 1999-11-18 01:04:00 137.750000 1999-11-18 01:14:00 118.000000 ... ... 1999-11-18 23:03:00 120.666667 1999-11-18 23:24:00 108.190476 1999-11-18 23:40:00 111.437500 1999-11-18 23:49:00 119.777778 1999-11-18 23:58:00 120.333333 <p>100 rows \u00d7 1 columns</p> <p>Let's visualize the windowed data over time:</p> <pre><code>fig, ax = plt.subplots()\n# gift_es.data.plot(ax=ax, marker='*')\nns_window_df.plot(figsize=(25,10), ax=ax, title='Average heart rates in between gift events', marker='*')\n# ax.legend(['gift rating', 'avg heart rate'])\n</code></pre> <pre>\n<code>&lt;Axes: title={'center': 'Average heart rates in between gift events'}&gt;</code>\n</pre> <p>Now we show how to use the <code>window</code> function from <code>TimeFrame</code> to do nonstationary windowing of the data. The <code>window</code> function in <code>TimeFrame</code> calls the <code>TimeSeries</code> function <code>nonstationary_window</code> that we called manually in the previous demonstration. Notice that we pass in <code>gift_es</code> to the <code>freq</code> parameter:</p> <pre><code>ns_window_timeframe = tf.window(freq=gift_es)\nns_window_timeframe\n</code></pre> gift_rating heart_rate 1999-11-18 00:11:00 0.676726 115.750000 1999-11-18 00:23:00 0.452966 115.250000 1999-11-18 01:00:00 0.384324 103.216216 1999-11-18 01:04:00 0.524974 137.750000 1999-11-18 01:14:00 0.618376 118.000000 ... ... ... 1999-11-18 23:03:00 0.164931 120.666667 1999-11-18 23:24:00 0.457748 108.190476 1999-11-18 23:40:00 0.726104 111.437500 1999-11-18 23:49:00 0.115720 119.777778 1999-11-18 23:58:00 0.044063 120.333333 <p>100 rows \u00d7 2 columns</p> <p>Now we plot the windowed heart rates against the rating for each gift: </p> <pre><code>ns_window_timeframe.plot(x='gift_rating', y='heart_rate', kind='scatter', ylim=[80,170])\n</code></pre> <pre>\n<code>&lt;Axes: xlabel='gift_rating', ylabel='heart_rate'&gt;</code>\n</pre>"},{"location":"birthday-gift-example/#example-applying-tigertail-to-birthday-gift-data","title":"Example: Applying TigerTail to Birthday Gift Data","text":"<p>In this notebook, we create 2 datasets and use them to demonstrate the use of the classes in the TigerTail library. The first dataset contains the timestamps at which Cassie receives a birthday gift on her birthday, along with a rating from 0-1 of how much she liked the gift. The second dataset contains Cassie's heart rate per minute of her birthday. All of this data is contained within the time range of 11/18/1999 00:00:00 to 11/19/1999 00:00:00.</p>"},{"location":"detecting-heavy-drinking-example/","title":"Detecting Heavy Drinking Example","text":"<p>First we import the necessary libraries:</p> <pre><code>import sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\nsys.path.insert(0, '../TT')\nfrom tigertail import TimeFrame, TimeSeries, EventSeries\n</code></pre> <p>Then we load in the TAC data for subject SA0297 (we pick one subject at random for demo purposes).</p> <pre><code>SA0297_TAC_path = '../data/bar-crawl-detecting-heavy-drinking/clean_tac/SA0297_clean_TAC.csv'\nSA0297_TAC_df = pd.read_csv(SA0297_TAC_path).set_index('timestamp')\n# convert epoch time to datetime\nSA0297_TAC_df.index = [datetime.fromtimestamp(i) for i in SA0297_TAC_df.index]\n# SA0297_TAC_df.index.name = None\nSA0297_TAC_df\n</code></pre> TAC_Reading 2017-05-02 11:18:43 -0.010229 2017-05-02 11:49:06 -0.002512 2017-05-02 11:54:23 0.003249 2017-05-02 11:59:39 0.005404 2017-05-02 12:04:55 0.003377 2017-05-02 12:10:11 -0.001451 2017-05-02 12:15:27 -0.005816 2017-05-02 12:20:43 -0.006248 2017-05-02 12:25:59 -0.001631 2017-05-02 12:31:15 0.005332 2017-05-02 12:36:31 0.009407 2017-05-02 12:41:47 0.006597 2017-05-02 13:12:05 -0.002201 2017-05-02 13:42:22 -0.010741 2017-05-02 14:12:39 -0.011314 2017-05-02 14:42:57 -0.000636 2017-05-02 15:13:22 0.016699 2017-05-02 15:43:44 0.030566 2017-05-02 16:14:08 0.032672 2017-05-02 16:44:33 0.023972 2017-05-02 17:15:00 0.016196 2017-05-02 17:45:42 0.025131 2017-05-02 18:16:23 0.059263 2017-05-02 18:46:47 0.111503 2017-05-02 19:17:29 0.160592 2017-05-02 19:48:11 0.182644 2017-05-02 20:18:58 0.166101 2017-05-02 20:49:45 0.120321 2017-05-02 21:20:27 0.071503 2017-05-02 21:51:11 0.047758 2017-05-02 22:21:55 0.062414 2017-05-02 22:52:22 0.106077 2017-05-02 23:23:06 0.152476 2017-05-02 23:53:57 0.174293 2017-05-03 00:24:42 0.158992 2017-05-03 00:55:26 0.114999 2017-05-03 01:26:11 0.065172 2017-05-03 01:56:57 0.032527 2017-05-03 02:27:42 0.027547 2017-05-03 02:58:26 0.044422 2017-05-03 03:29:10 0.067225 2017-05-03 03:59:52 0.080797 2017-05-03 04:30:33 0.079004 2017-05-03 05:01:15 0.065889 2017-05-03 05:31:59 0.050462 2017-05-03 06:02:39 0.039745 2017-05-03 06:33:20 0.034904 2017-05-03 07:04:00 0.032288 2017-05-03 07:34:41 0.027591 2017-05-03 08:05:05 0.019586 2017-05-03 08:35:29 0.010822 2017-05-03 09:05:53 0.005265 2017-05-03 09:36:16 0.004950 2017-05-03 10:06:39 0.008145 2017-05-03 10:37:02 0.010223 2017-05-03 11:07:25 0.006509 2017-05-03 11:37:51 -0.004768 2017-05-03 12:08:14 -0.021275 <p>Now we load the accelerometer data into a DataFrame and set the timestamps as the index. The timestamps require a bit of preprocessing as seen in the try-except block.</p> <pre><code>accelerometer_path = '../data/bar-crawl-detecting-heavy-drinking/all_accelerometer_data_pids_13.csv'\naccelerometer_df = pd.read_csv(accelerometer_path).set_index('time')\ntry:\n    accelerometer_df.index = [datetime.fromtimestamp(i) for i in accelerometer_df.index]\nexcept ValueError:\n    accelerometer_df.index = [datetime.fromtimestamp(i/1000) for i in accelerometer_df.index]\naccelerometer_df\n</code></pre> pid x y z 1970-01-01 01:00:00.000 JB3156 0.000000 0.000000 0.000000 1970-01-01 01:00:00.000 CC6740 0.000000 0.000000 0.000000 2017-05-02 16:04:42.409 SA0297 0.075800 0.027300 -0.010200 2017-05-02 16:04:42.455 SA0297 -0.035900 0.079400 0.003700 2017-05-02 16:04:42.500 SA0297 -0.242700 -0.086100 -0.016300 ... ... ... ... ... 2017-05-03 18:34:08.196 CC6740 -0.133956 0.124726 -0.010736 2017-05-03 18:34:08.220 CC6740 -0.100764 0.180872 0.046449 2017-05-03 18:34:08.245 CC6740 -0.131853 0.195934 0.181088 2017-05-03 18:34:08.270 CC6740 -0.149704 0.194482 0.202393 2017-05-03 18:34:08.294 CC6740 -0.107288 0.153548 0.168595 <p>14057567 rows \u00d7 4 columns</p> <p>We filter the accelerometer DataFrame to just the entries for subject SA0297.</p> <pre><code>SA0297_mask = accelerometer_df['pid'] == 'SA0297'\nSA0297_accelerometer_df = pd.DataFrame(accelerometer_df[SA0297_mask]).drop(['pid'], axis=1)\nSA0297_accelerometer_df\n</code></pre> x y z 2017-05-02 16:04:42.409 0.0758 0.0273 -0.0102 2017-05-02 16:04:42.455 -0.0359 0.0794 0.0037 2017-05-02 16:04:42.500 -0.2427 -0.0861 -0.0163 2017-05-02 16:04:43.945 -0.2888 0.0514 -0.0145 2017-05-02 16:04:43.953 -0.0413 -0.0184 -0.0105 ... ... ... ... 2017-05-03 17:18:55.596 -0.0107 0.0801 0.0840 2017-05-03 17:18:55.641 0.0132 0.0835 0.0493 2017-05-03 17:18:55.686 0.0480 -0.0071 0.1374 2017-05-03 17:18:55.731 0.0597 -0.0426 -0.1090 2017-05-03 17:18:55.776 -0.0061 -0.0700 -0.0018 <p>962901 rows \u00d7 3 columns</p> <p>The following cell removes some duplicate entries from the DataFrame just created:</p> <pre><code>SA0297_accelerometer_df = SA0297_accelerometer_df.loc[~SA0297_accelerometer_df.index.duplicated(keep='first')]\nSA0297_accelerometer_df\n</code></pre> x y z 2017-05-02 16:04:42.409 0.0758 0.0273 -0.0102 2017-05-02 16:04:42.455 -0.0359 0.0794 0.0037 2017-05-02 16:04:42.500 -0.2427 -0.0861 -0.0163 2017-05-02 16:04:43.945 -0.2888 0.0514 -0.0145 2017-05-02 16:04:43.953 -0.0413 -0.0184 -0.0105 ... ... ... ... 2017-05-03 17:18:55.596 -0.0107 0.0801 0.0840 2017-05-03 17:18:55.641 0.0132 0.0835 0.0493 2017-05-03 17:18:55.686 0.0480 -0.0071 0.1374 2017-05-03 17:18:55.731 0.0597 -0.0426 -0.1090 2017-05-03 17:18:55.776 -0.0061 -0.0700 -0.0018 <p>962891 rows \u00d7 3 columns</p> <p>We now create 2 TimeSeries objects using the TAC and accelerometer DataFrames. We also visualize the datasets as they are.</p> <pre><code>TAC_ts = TimeSeries(SA0297_TAC_df, agg_func=np.max)\naccelerometer_ts = TimeSeries(SA0297_accelerometer_df, agg_func=np.mean)\n\nTAC_ts.data.plot(marker=\"*\")\n</code></pre> <pre>\n<code>&lt;Axes: &gt;</code>\n</pre> <p>We plot the three axes of accelerometer position data on three separate plots so that it's easier to get an idea of what's going on overall over time.</p> <pre><code>fig, axs = plt.subplots(3, 1, figsize=(20, 20))\naxs[0].plot(accelerometer_ts.data.index, accelerometer_ts.data['x'])\naxs[0].set_title('x coordinate over time')\naxs[1].plot(accelerometer_ts.data.index, accelerometer_ts.data['y'], 'tab:orange')\naxs[1].set_title('y coordinate over time')\naxs[2].plot(accelerometer_ts.data.index, accelerometer_ts.data['z'], 'tab:green')\naxs[2].set_title('z coordinate over time')\nplt.show()\n</code></pre> <p>Now we create a TimeFrame object containing the TAC and accelerometer TimeSeries objects so that we can try windowing the datasets together. Since the TAC data measurements occur much more sparsely than the much finer accelerometer data, we choose to window both datasets using hour long intervals to better compare them.</p> <pre><code>tf = TimeFrame()\ntf['TAC_ts'] = TAC_ts\ntf['accelerometer_ts'] = accelerometer_ts\n\nwindow_tf = tf.window('1h')\nwindow_tf\n</code></pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[8], line 5\n      2 tf['TAC_ts'] = TAC_ts\n      3 tf['accelerometer_ts'] = accelerometer_ts\n----&gt; 5 window_tf = tf.window('1h')\n      6 window_tf\n\nFile ~/Desktop/TigerTail/examples/../TT/tigertail.py:68, in TimeFrame.window(self, freq, fillnan)\n     66 if fillnan == True:\n     67     windowed_df = pd.concat(grouped_by, axis=1).fillna(0)\n---&gt; 68     windowed_df.columns = names\n     69     return windowed_df\n     70 else:\n\nFile ~/Desktop/TigerTail/.conda/lib/python3.11/site-packages/pandas/core/generic.py:6313, in NDFrame.__setattr__(self, name, value)\n   6311 try:\n   6312     object.__getattribute__(self, name)\n-&gt; 6313     return object.__setattr__(self, name, value)\n   6314 except AttributeError:\n   6315     pass\n\nFile properties.pyx:69, in pandas._libs.properties.AxisProperty.__set__()\n\nFile ~/Desktop/TigerTail/.conda/lib/python3.11/site-packages/pandas/core/generic.py:814, in NDFrame._set_axis(self, axis, labels)\n    809 \"\"\"\n    810 This is called from the cython code when we set the `index` attribute\n    811 directly, e.g. `series.index = [1, 2, 3]`.\n    812 \"\"\"\n    813 labels = ensure_index(labels)\n--&gt; 814 self._mgr.set_axis(axis, labels)\n    815 self._clear_item_cache()\n\nFile ~/Desktop/TigerTail/.conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:238, in BaseBlockManager.set_axis(self, axis, new_labels)\n    236 def set_axis(self, axis: AxisInt, new_labels: Index) -&gt; None:\n    237     # Caller is responsible for ensuring we have an Index object.\n--&gt; 238     self._validate_set_axis(axis, new_labels)\n    239     self.axes[axis] = new_labels\n\nFile ~/Desktop/TigerTail/.conda/lib/python3.11/site-packages/pandas/core/internals/base.py:98, in DataManager._validate_set_axis(self, axis, new_labels)\n     95     pass\n     97 elif new_len != old_len:\n---&gt; 98     raise ValueError(\n     99         f\"Length mismatch: Expected axis has {old_len} elements, new \"\n    100         f\"values have {new_len} elements\"\n    101     )\n\nValueError: Length mismatch: Expected axis has 2 elements, new values have 4 elements</pre> <p>We make a plot of the windowed data to visualize the relationship between the TAC readings and accelerometer readings.</p> <pre><code>window_tf.plot(marker='*')\n</code></pre> <pre>\n<code>&lt;Axes: &gt;</code>\n</pre> <p>We can see hours at which the average TAC spikes, so we would want to analyze and better visualize trends in the accelerometer position data during those times in order to predict periods of drunkenness from accelerometer data. We could add more functionality to the TimeFrame window function so that we can window the datasets together based on periods of high TAC readings, rather than just default frequencies.</p>"},{"location":"detecting-heavy-drinking-example/#example-applying-tigertail-to-bar-crawl-detecting-heavy-drinking-data","title":"Example: Applying TigerTail to Bar Crawl: Detecting Heavy Drinking Data","text":"<p>In this notebook, we apply the TigerTail library to the \"Bar Crawl: Detecting Heavy Drinking\" data from https://archive.ics.uci.edu/dataset/515/bar+crawl+detecting+heavy+drinking. There are 3 datasets being used here:</p> <ul> <li>all_accelerometer_data_pids13.csv: 3 axes of accelerometer data tracking subject position (time series data)</li> <li>phone_types.csv: subject phone types</li> <li>clean_tac contains a .csv file of TAC (transdermal alcohol content) measurements for each subject (time series data)</li> </ul> <p>The eventual goal is to predict times when a subject's TAC is high enough to indicate that they are drunk (based on a predetermined threshold), given a sample of accelerometer data. This notebook shows how to use TigerTail classes to help visualize patterns in the accelerometer data leading up to a occurrence of drunkenness.</p>"},{"location":"reference/","title":"Reference","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of the <code>TigerTail</code> library code.</p> <p>               Bases: <code>MutableMapping</code></p> <p>The TimeFrame class follows the structure of a pandas DataFrame and is meant to contain TimeSeries and EventSeries objects.  It inherits from the MutableMapping class to be a mapping that works like both a dict and a mutable object,  i.e. d = D(foo='bar')     d.foo returns 'bar'.</p> <p>Methods:</p> Name Description <code>__setitem__</code> <p>Sets a key-value pair.</p> <code>__getitem__</code> <p>Returns the value associated with a key.</p> <code>__delitem__</code> <p>Deletes a key-value pair.</p> <code>__iter__</code> <p>Returns an iterator based on self.dict</p> <code>__len__</code> <p>Returns the length of the dict mapping.</p> <code>__str__</code> <p>Returns a string representation of the dict mapping.</p> <code>__repr__</code> <p>Returns a string containing the class, id, &amp; reproducible representation in the REPL.</p> <code>window</code> <p>Windows the data of each TimeSeries or EventSeries object in self.dict according to the input frequency and      concatenates them into a single DataFrame.</p> Source code in <code>TT/tigertail.py</code> <pre><code>class TimeFrame(MutableMapping):\n    \"\"\"\n    The TimeFrame class follows the structure of a pandas DataFrame and is meant to contain TimeSeries and EventSeries objects. \n    It inherits from the MutableMapping class to be a mapping that works like both a dict and a mutable object, \n    i.e. d = D(foo='bar')\n        d.foo returns 'bar'.\n\n    Methods:\n        __setitem__: Sets a key-value pair.\n        __getitem__: Returns the value associated with a key.\n        __delitem__: Deletes a key-value pair.\n        __iter__: Returns an iterator based on self.__dict__\n        __len__: Returns the length of the dict mapping.\n        __str__: Returns a string representation of the dict mapping.\n        __repr__: Returns a string containing the class, id, &amp; reproducible representation in the REPL.\n        window: Windows the data of each TimeSeries or EventSeries object in self.__dict__ according to the input frequency and \n                concatenates them into a single DataFrame.\n\n    Attributes:\n        N/A\n    \"\"\"\n    # ``__init__`` method required to create instance from class.\n    def __init__(self, *args, **kwargs):\n        '''Use the object dict'''\n        self.__dict__.update(*args, **kwargs)\n\n    # The next five methods are requirements of the ABC.\n    def __setitem__(self, key, value):\n        \"\"\"\n        __setitem__ is used for setting a key-value pair for the TimeFrame dict.\n\n        Args:\n            key (str): String for the dict key.\n            value: Value to pair with the key.\n\n        Returns: N/A\n        \"\"\"\n        self.__dict__[key] = value\n\n    def __getitem__(self, key):\n        \"\"\"\n        __getitem__ is used for getting the value associated with the inputted key from the TimeFrame dict.\n\n        Args:\n            key (str): String for the dict key.\n\n        Returns: self.__dict__[key]\n        \"\"\"\n        return self.__dict__[key]\n\n    def __delitem__(self, key):\n        \"\"\"\n        __delitem__ is used for deleting a key-value pair for the dict representation of the mapping.\n\n        Args:\n            key (str): String for the dict key.\n\n        Returns: N/A\n        \"\"\"\n        del self.__dict__[key]\n\n    def __iter__(self):\n        \"\"\"\n        __iter__ is used to generate an iterator using self.__dict__.\n\n        Args: N/A\n\n        Returns: Iterator object\n        \"\"\"\n        return iter(self.__dict__)\n\n    def __len__(self):\n        \"\"\"\n        __len__ is used for getting the length of the dict representation of the mapping.\n\n        Args: N/A\n\n        Returns: The length of self.__dict__.\n        \"\"\"\n        return len(self.__dict__)\n\n    # The final two methods aren't required, but nice for demo purposes:\n    def __str__(self):\n        \"\"\"\n        __str__ returns a simple dict representation of the mapping.\n\n        Args: N/A\n\n        Returns: String version of self.__dict__.\n        \"\"\"\n        return str(self.__dict__)\n\n    def __repr__(self):\n        \"\"\"\n        __repr__ returns a string containing the class, id, &amp; reproducible representation in the REPL.\n\n        Args: N/A\n\n        Returns: String containing the class, id, &amp; reproducible representation in the REPL.\n        \"\"\"\n        # TODO : remake this to print out the minimum window available\n        #        to put all the data together\n        return '{}, D({})'.format(super(D, self).__repr__(), \n                                  self.__dict__)\n\n    def window(self, freq, fillnan=True):\n        \"\"\"\n        The window function windows all the time and event series data stored in the object according to either a certain uniform time or event timestamps. It first calls the window function of each \n        TimeSeries or EventSeries object. It then concatenates all of the returned windowed DataFrames together.\n\n        Args:\n            freq (str or EventSeries): Either a string representing the uniform length of time to window by (i.e. '1h' for one hour), or an EventSeries object containing the data to be used for windowing.\n                                        See here for list of default frequencies: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\n            fillnan (bool, default True): Determines whether NaN values in the result DataFrame are replaced by 0s.\n\n        Returns:\n            windowed_df: A DataFrame of the concatenated windowed data.\n        \"\"\"\n        grouped_by = []\n        for ts in self.__dict__:\n            if freq.__class__ == EventSeries and self.__dict__[ts].__class__ == TimeSeries:\n                grouped_by.append(self.__dict__[ts].nonstationary_window(es=freq, fillnan=True))\n            elif freq.__class__ == EventSeries and self.__dict__[ts].__class__ == EventSeries:\n                grouped_by.append(self.__dict__[ts].data)\n            else:\n                grouped_by.append(self.__dict__[ts].window(freq=freq, fillnan=True))\n\n        names = sum([[col for col in self.__dict__[ts].data.columns] for ts in self.__dict__], [])\n\n        if fillnan == True:\n            windowed_df = pd.concat(grouped_by, axis=1).fillna(0)\n            windowed_df.columns = names\n            return windowed_df\n        else:\n            windowed_df = pd.concat(grouped_by, axis=1)\n            windowed_df.columns = names\n            return windowed_df\n</code></pre>"},{"location":"reference/#TT.tigertail.TimeFrame.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>delitem is used for deleting a key-value pair for the dict representation of the mapping.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>String for the dict key.</p> required <p>Returns: N/A</p> Source code in <code>TT/tigertail.py</code> <pre><code>def __delitem__(self, key):\n    \"\"\"\n    __delitem__ is used for deleting a key-value pair for the dict representation of the mapping.\n\n    Args:\n        key (str): String for the dict key.\n\n    Returns: N/A\n    \"\"\"\n    del self.__dict__[key]\n</code></pre>"},{"location":"reference/#TT.tigertail.TimeFrame.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>getitem is used for getting the value associated with the inputted key from the TimeFrame dict.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>String for the dict key.</p> required <p>Returns: self.dict[key]</p> Source code in <code>TT/tigertail.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"\n    __getitem__ is used for getting the value associated with the inputted key from the TimeFrame dict.\n\n    Args:\n        key (str): String for the dict key.\n\n    Returns: self.__dict__[key]\n    \"\"\"\n    return self.__dict__[key]\n</code></pre>"},{"location":"reference/#TT.tigertail.TimeFrame.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Use the object dict</p> Source code in <code>TT/tigertail.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    '''Use the object dict'''\n    self.__dict__.update(*args, **kwargs)\n</code></pre>"},{"location":"reference/#TT.tigertail.TimeFrame.__iter__","title":"<code>__iter__()</code>","text":"<p>iter is used to generate an iterator using self.dict.</p> <p>Args: N/A</p> <p>Returns: Iterator object</p> Source code in <code>TT/tigertail.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    __iter__ is used to generate an iterator using self.__dict__.\n\n    Args: N/A\n\n    Returns: Iterator object\n    \"\"\"\n    return iter(self.__dict__)\n</code></pre>"},{"location":"reference/#TT.tigertail.TimeFrame.__len__","title":"<code>__len__()</code>","text":"<p>len is used for getting the length of the dict representation of the mapping.</p> <p>Args: N/A</p> <p>Returns: The length of self.dict.</p> Source code in <code>TT/tigertail.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    __len__ is used for getting the length of the dict representation of the mapping.\n\n    Args: N/A\n\n    Returns: The length of self.__dict__.\n    \"\"\"\n    return len(self.__dict__)\n</code></pre>"},{"location":"reference/#TT.tigertail.TimeFrame.__repr__","title":"<code>__repr__()</code>","text":"<p>repr returns a string containing the class, id, &amp; reproducible representation in the REPL.</p> <p>Args: N/A</p> <p>Returns: String containing the class, id, &amp; reproducible representation in the REPL.</p> Source code in <code>TT/tigertail.py</code> <pre><code>def __repr__(self):\n    \"\"\"\n    __repr__ returns a string containing the class, id, &amp; reproducible representation in the REPL.\n\n    Args: N/A\n\n    Returns: String containing the class, id, &amp; reproducible representation in the REPL.\n    \"\"\"\n    # TODO : remake this to print out the minimum window available\n    #        to put all the data together\n    return '{}, D({})'.format(super(D, self).__repr__(), \n                              self.__dict__)\n</code></pre>"},{"location":"reference/#TT.tigertail.TimeFrame.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>setitem is used for setting a key-value pair for the TimeFrame dict.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>String for the dict key.</p> required <code>value</code> <p>Value to pair with the key.</p> required <p>Returns: N/A</p> Source code in <code>TT/tigertail.py</code> <pre><code>def __setitem__(self, key, value):\n    \"\"\"\n    __setitem__ is used for setting a key-value pair for the TimeFrame dict.\n\n    Args:\n        key (str): String for the dict key.\n        value: Value to pair with the key.\n\n    Returns: N/A\n    \"\"\"\n    self.__dict__[key] = value\n</code></pre>"},{"location":"reference/#TT.tigertail.TimeFrame.__str__","title":"<code>__str__()</code>","text":"<p>str returns a simple dict representation of the mapping.</p> <p>Args: N/A</p> <p>Returns: String version of self.dict.</p> Source code in <code>TT/tigertail.py</code> <pre><code>def __str__(self):\n    \"\"\"\n    __str__ returns a simple dict representation of the mapping.\n\n    Args: N/A\n\n    Returns: String version of self.__dict__.\n    \"\"\"\n    return str(self.__dict__)\n</code></pre>"},{"location":"reference/#TT.tigertail.TimeFrame.window","title":"<code>window(freq, fillnan=True)</code>","text":"<p>The window function windows all the time and event series data stored in the object according to either a certain uniform time or event timestamps. It first calls the window function of each  TimeSeries or EventSeries object. It then concatenates all of the returned windowed DataFrames together.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>str or EventSeries</code> <p>Either a string representing the uniform length of time to window by (i.e. '1h' for one hour), or an EventSeries object containing the data to be used for windowing.                         See here for list of default frequencies: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases</p> required <code>fillnan</code> <code>bool, default True</code> <p>Determines whether NaN values in the result DataFrame are replaced by 0s.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>windowed_df</code> <p>A DataFrame of the concatenated windowed data.</p> Source code in <code>TT/tigertail.py</code> <pre><code>def window(self, freq, fillnan=True):\n    \"\"\"\n    The window function windows all the time and event series data stored in the object according to either a certain uniform time or event timestamps. It first calls the window function of each \n    TimeSeries or EventSeries object. It then concatenates all of the returned windowed DataFrames together.\n\n    Args:\n        freq (str or EventSeries): Either a string representing the uniform length of time to window by (i.e. '1h' for one hour), or an EventSeries object containing the data to be used for windowing.\n                                    See here for list of default frequencies: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\n        fillnan (bool, default True): Determines whether NaN values in the result DataFrame are replaced by 0s.\n\n    Returns:\n        windowed_df: A DataFrame of the concatenated windowed data.\n    \"\"\"\n    grouped_by = []\n    for ts in self.__dict__:\n        if freq.__class__ == EventSeries and self.__dict__[ts].__class__ == TimeSeries:\n            grouped_by.append(self.__dict__[ts].nonstationary_window(es=freq, fillnan=True))\n        elif freq.__class__ == EventSeries and self.__dict__[ts].__class__ == EventSeries:\n            grouped_by.append(self.__dict__[ts].data)\n        else:\n            grouped_by.append(self.__dict__[ts].window(freq=freq, fillnan=True))\n\n    names = sum([[col for col in self.__dict__[ts].data.columns] for ts in self.__dict__], [])\n\n    if fillnan == True:\n        windowed_df = pd.concat(grouped_by, axis=1).fillna(0)\n        windowed_df.columns = names\n        return windowed_df\n    else:\n        windowed_df = pd.concat(grouped_by, axis=1)\n        windowed_df.columns = names\n        return windowed_df\n</code></pre>"},{"location":"serprate-ai-example/","title":"SerpRate AI Example","text":"<p>First we import the necessary libraries and load in the bubble data:</p> <pre><code>import sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsys.path.insert(0, '../TT')\nfrom tigertail import TimeFrame, TimeSeries, EventSeries\n</code></pre> <p>The bubbles dataset was constructed by using a detected bubble as a template and then used a matched filter template matching algorithm to find the other bubbles. The columns in the dataset are <code>similarity</code>, <code>template_id</code>, and <code>ones</code>. <code>template_id</code> was the id for the template used, which is just all 0's. <code>similarity</code> is the calculated correlation between the detected bubble and the template bubble. <code>ones</code> is just a column of 1's which is helpful for counting the number of bubbles over time for plotting or windowing purposes.</p> <pre><code>bubbles_path = '../data/serprate-ai/bubbles.csv'\nbubbles_df = pd.read_csv(bubbles_path).set_index('time')\nbubbles_df.index = pd.to_datetime(bubbles_df.index, format='ISO8601')\nbubbles_df.index.name = None\nbubbles_df = bubbles_df.tz_localize(None)\nbubbles_df\n</code></pre> similarity template_id ones 2019-05-07 16:37:35.302001 0.800337 0 1 2019-05-17 00:56:44.213000 0.801590 0 1 2019-05-18 11:45:45.948000 0.948577 0 1 2019-05-18 11:47:16.325000 0.962242 0 1 2019-05-18 11:47:54.450000 0.964003 0 1 ... ... ... ... 2020-02-02 05:41:40.738000 0.811049 0 1 2020-02-02 08:03:25.875000 0.804105 0 1 2020-02-02 12:32:29.838000 0.810579 0 1 2020-02-02 22:43:12.641000 0.812358 0 1 2020-02-03 09:30:09.057000 0.800934 0 1 <p>2434 rows \u00d7 3 columns</p> <pre><code># drop the template_id column since it's not necessary for this demonstration\n\nbubbles_es = EventSeries(bubbles_df.drop(columns=['template_id']), agg_func=np.sum)\nbubbles_es.window('1D').plot(y='ones', marker='*')\n</code></pre> <pre>\n<code>/Users/cassandralem/Desktop/TigerTail/.conda/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:84: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n  return reduction(axis=axis, out=out, **passkwargs)\n</code>\n</pre> <pre>\n<code>&lt;Axes: &gt;</code>\n</pre> <p>Now we load the daily precipitation dataset into a DataFrame. We convert the timestamp column values to datetime objects and then set that column as the index.</p> <pre><code>daily_precip_path = '../data/serprate-ai/daily_precipitation.csv'\ndaily_precip_df = pd.read_csv(daily_precip_path).set_index('system:time_start')\ndaily_precip_df.index = pd.to_datetime(daily_precip_df.index)\ndaily_precip_df.index.name = None\n# daily_precip_df = daily_precip_df.tz_localize(None)\ndaily_precip_df\n</code></pre> total_precipitation_sum 2019-01-01 0.0 2019-01-02 0.0 2019-01-03 0.0 2019-01-04 0.0 2019-01-05 0.0 ... ... 2020-02-25 0.0 2020-02-26 0.0 2020-02-27 0.0 2020-02-28 0.0 2020-02-29 0.0 <p>425 rows \u00d7 1 columns</p> <p>Now we create a TimeSeries object with the created DataFrame and demonstrate the use of the TimeSeries window function. We window the data by finding the averages over periods of 1 week and plot the results.</p> <pre><code>daily_precip_ts = TimeSeries(daily_precip_df, agg_func=np.mean)\ndaily_precip_ts.window('1W').plot(marker='*')\n</code></pre> <pre>\n<code>&lt;Axes: &gt;</code>\n</pre> <pre><code>daily_precip_ts.window('1D')\n</code></pre> <pre>\n<code>2019-01-01    0.0\n2019-01-02    0.0\n2019-01-03    0.0\n2019-01-04    0.0\n2019-01-05    0.0\n             ... \n2020-02-25    0.0\n2020-02-26    0.0\n2020-02-27    0.0\n2020-02-28    0.0\n2020-02-29    0.0\nFreq: D, Length: 425, dtype: float64</code>\n</pre> <p>We do the same process with the hourly surface pressure dataset, but instead window the data according to 1 day intervals:</p> <pre><code>hourly_sp_path = '../data/serprate-ai/hourly_surface_pressure.csv'\nhourly_sp_df = pd.read_csv(hourly_sp_path).set_index('datetime')\nhourly_sp_df.index.name = None\nhourly_sp_df.index = pd.to_datetime(hourly_sp_df.index)\nhourly_sp_df = hourly_sp_df.tz_localize(None)\nhourly_sp_df\n</code></pre> surface_pressure 2017-01-01 00:00:00 96536.368 2017-01-01 01:00:00 96553.223 2017-01-01 02:00:00 96599.012 2017-01-01 03:00:00 96674.992 2017-01-01 04:00:00 96747.476 ... ... 2020-12-31 19:00:00 96888.352 2020-12-31 20:00:00 96892.005 2020-12-31 21:00:00 96861.655 2020-12-31 22:00:00 96875.097 2020-12-31 23:00:00 96866.614 <p>35064 rows \u00d7 1 columns</p> <pre><code>hourly_sp_ts = TimeSeries(hourly_sp_df, agg_func=np.mean)\nhourly_sp_ts.window('1D').plot(marker='*')\n</code></pre> <pre>\n<code>&lt;Axes: &gt;</code>\n</pre> <p>Now we can create a TimeFrame object using the bubble EventSeries object and two TimeSeries objects we already created.</p> <pre><code>tf = TimeFrame()\ntf['bubbles_es'] = bubbles_es\ntf['daily_precip_ts'] = daily_precip_ts\ntf['hourly_sp_ts'] = hourly_sp_ts\n</code></pre> <p>Then we can use the TimeFrame window function to window all 3 datasets according to 1 day intervals (but using their own object-specific aggregation functions, so we have the daily counts of bubbles and daily averages of precipitation and surface pressure):</p> <pre><code>tf_day_window = tf.window('1D')\ntf_day_window\n</code></pre> <pre>\n<code>/Users/cassandralem/Desktop/TigerTail/.conda/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:84: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n  return reduction(axis=axis, out=out, **passkwargs)\n</code>\n</pre> similarity ones total_precipitation_sum surface_pressure 2017-01-01 0.0 0.0 0.0 96671.014625 2017-01-02 0.0 0.0 0.0 96779.818708 2017-01-03 0.0 0.0 0.0 96759.870500 2017-01-04 0.0 0.0 0.0 96655.596208 2017-01-05 0.0 0.0 0.0 96577.570625 ... ... ... ... ... 2020-12-27 0.0 0.0 0.0 97110.352208 2020-12-28 0.0 0.0 0.0 96998.292167 2020-12-29 0.0 0.0 0.0 96908.148167 2020-12-30 0.0 0.0 0.0 96925.027333 2020-12-31 0.0 0.0 0.0 96878.317208 <p>1461 rows \u00d7 4 columns</p> <p>We visualize the windowing of the 3 datasets:</p> <pre><code>fig, axs = plt.subplots(3, 1, figsize=(20, 20))\naxs[0].plot(tf_day_window.index, tf_day_window['similarity'])\naxs[0].set_title('Number of bubbles per day')\naxs[1].plot(tf_day_window.index, tf_day_window['total_precipitation_sum'], 'tab:orange')\naxs[1].set_title('Average precipitation per day')\naxs[2].plot(tf_day_window.index, tf_day_window['surface_pressure'], 'tab:green')\naxs[2].set_title('Average surface pressure per day')\nplt.show()\n</code></pre> <p>Let's view part of the plot where the bubble count goes up closer.</p> <pre><code>shorter_period_df = tf_day_window.loc['2019-01-01':'2020-02-28']\nshorter_period_df\n</code></pre> similarity ones total_precipitation_sum surface_pressure 2019-01-01 0.0 0.0 0.0 97144.518375 2019-01-02 0.0 0.0 0.0 97132.137125 2019-01-03 0.0 0.0 0.0 97115.655250 2019-01-04 0.0 0.0 0.0 96960.913083 2019-01-05 0.0 0.0 0.0 96918.480292 ... ... ... ... ... 2020-02-24 0.0 0.0 0.0 96773.375458 2020-02-25 0.0 0.0 0.0 96625.853250 2020-02-26 0.0 0.0 0.0 96574.696375 2020-02-27 0.0 0.0 0.0 96496.441875 2020-02-28 0.0 0.0 0.0 96356.504083 <p>424 rows \u00d7 4 columns</p> <pre><code>fig, axs = plt.subplots(3, 1, figsize=(20, 20))\naxs[0].plot(shorter_period_df.index, shorter_period_df['similarity'])\naxs[0].set_title('Number of bubbles per day')\naxs[1].plot(shorter_period_df.index, shorter_period_df['total_precipitation_sum'], 'tab:orange')\naxs[1].set_title('Average precipitation per day')\naxs[2].plot(shorter_period_df.index, shorter_period_df['surface_pressure'], 'tab:green')\naxs[2].set_title('Average surface pressure per day')\nplt.show()\n</code></pre> <p>We can see days at which there are a large number of bubbles that formed, so we would want to look at trends in the other time series datasets, including the precipitation and surface pressure data, leading up to or surrounding days with a large number of bubbles. We now demonstrate how to use the TimeFrame window function to window the precipitation and surface pressure data according to the bubble event timestamps.</p> <pre><code>ns_window_df = tf.window(freq=bubbles_es)\nns_window_df\n</code></pre> similarity ones total_precipitation_sum surface_pressure 2019-05-07 16:37:35.302001 0.800337 1 0.000000 0.000000 2019-05-17 00:56:44.213000 0.801590 1 0.000252 96162.925114 2019-05-18 11:45:45.948000 0.948577 1 0.006000 95953.806629 2019-05-18 11:47:16.325000 0.962242 1 0.000000 0.000000 2019-05-18 11:47:54.450000 0.964003 1 0.000000 0.000000 ... ... ... ... ... 2020-02-02 05:41:40.738000 0.811049 1 0.000000 97002.704800 2020-02-02 08:03:25.875000 0.804105 1 0.000000 97063.475333 2020-02-02 12:32:29.838000 0.810579 1 0.000000 96804.011000 2020-02-02 22:43:12.641000 0.812358 1 0.000000 97010.311500 2020-02-03 09:30:09.057000 0.800934 1 0.000000 97048.549818 <p>2434 rows \u00d7 4 columns</p> <p>Now we can make a plot to see what combinations of average precipitation and surface pressure are observed during the intervals in between bubble events.</p> <pre><code>ns_window_df.plot(x='total_precipitation_sum', y='surface_pressure', kind='scatter', ylim=[94000, 98000])\n</code></pre> <pre>\n<code>&lt;Axes: xlabel='total_precipitation_sum', ylabel='surface_pressure'&gt;</code>\n</pre>"},{"location":"serprate-ai-example/#example-applying-tigertail-to-serprateai-data","title":"Example: Applying TigerTail to SerpRateAI Data","text":"<p>In this notebook, we apply the TigerTail library to the SerpRateAI data that can be downloaded from https://github.com/SerpRateAI/datasets. This repository contains a number of datasets related to processes that lead to gas bubble formation in rock at an Oman drilling site.</p> <p>The eventual goal is to predict when the formation of bubbles occur using the other time series datasets. This notebook shows how to use TigerTail classes to help visualize patterns in the time series datasets leading up to a bubble formation event. The specific datasets used in this notebook are:</p> <ul> <li>bubbles.csv: contains the timestamps at which bubbles were detected</li> <li>daily_precipitation.csv: contains the amount of precipitation per day in the area (not cumulative)</li> <li>hourly_surface_pressure.csv: contains surface pressure measurements per hour (not cumulative)</li> </ul>"},{"location":"student-example/","title":"Student Example","text":"<p>First we import the necessary libraries and load in the data:</p> <pre><code>import sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsys.path.insert(0, '../TT')\nfrom tigertail import TimeFrame, TimeSeries, EventSeries\n</code></pre> <pre><code>data_path = '../data/anon_student_data.csv'\nstudent_df = pd.read_csv(data_path)\nstudent_df\n</code></pre> first_course_year semester_idx female white_asian start_age first_gen HS_Zip_Code changed_major n_majors total_semester_credit_hours ... Geography Median income (dollars); Estimate; Households Median income (dollars); Margin of Error; Households Number; Estimate; FAMILIES - Families Number; Margin of Error; FAMILIES - Families Median income (dollars); Estimate; FAMILIES - Families Median income (dollars); Margin of Error; FAMILIES - Families zipstr zip_median_income_log anon_id 0 13.0 1.0 1.0 1.0 19.0 0.0 NaN 0.0 1.0 14.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 125713 1 17.0 1.0 1.0 1.0 18.0 1.0 48867 0.0 2.0 14.0 ... ZCTA5 48867 43388.0 2198.0 7417.0 211.0 56442.0 4490.0 48867.0 4.751602 160343 2 3.0 1.0 0.0 1.0 26.0 0.0 NaN 0.0 1.0 10.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 34721 3 0.0 1.0 0.0 1.0 18.0 0.0 34652 0.0 1.0 15.0 ... ZCTA5 34652 36329.0 3082.0 5958.0 369.0 48205.0 3885.0 34652.0 4.683092 6446 4 9.0 1.0 0.0 1.0 18.0 0.0 48346 0.0 1.0 15.0 ... ZCTA5 48346 74958.0 5627.0 6288.0 294.0 88802.0 7348.0 48346.0 4.948423 84932 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1553519 9.0 16.0 1.0 1.0 18.0 0.0 60030 0.0 1.0 13.0 ... ZCTA5 60030 83680.0 3258.0 9698.0 290.0 105239.0 4370.0 60030.0 5.022177 91426 1553520 8.0 16.0 1.0 1.0 18.0 0.0 48820 0.0 1.0 5.0 ... ZCTA5 48820 82964.0 3961.0 4905.0 238.0 96399.0 5734.0 48820.0 4.984073 78620 1553521 5.0 16.0 0.0 0.0 19.0 0.0 20011 0.0 1.0 6.0 ... ZCTA5 20011 65327.0 3261.0 13424.0 493.0 85369.0 8525.0 20011.0 4.931300 55141 1553522 16.0 16.0 1.0 1.0 19.0 1.0 48161 0.0 8.0 20.0 ... ZCTA5 48161 49627.0 3809.0 7110.0 251.0 61250.0 3619.0 48161.0 4.787106 152752 1553523 10.0 16.0 1.0 0.0 19.0 0.0 NaN 0.0 1.0 4.0 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 100695 <p>1553524 rows \u00d7 38 columns</p> <p>Now we create a dataframe with just the features <code>semester_idx</code>, <code>anon_id</code>, <code>major_gpa</code>, <code>cumulative_credit_hours</code>, and <code>nonmajor_credits</code>:</p> <pre><code>some_features_df = student_df[['semester_idx', 'anon_id', 'major_gpa', 'cumulative_credit_hours', 'nonmajor_credits']]\nsome_features_df\n</code></pre> semester_idx anon_id major_gpa cumulative_credit_hours nonmajor_credits 0 1.0 125713 0.000000 14.0 14.0 1 1.0 160343 1.500000 14.0 8.0 2 1.0 34721 2.333333 10.0 3.0 3 1.0 6446 0.000000 15.0 15.0 4 1.0 84932 1.300000 15.0 12.0 ... ... ... ... ... ... 1553519 16.0 91426 1.500000 138.0 6.0 1553520 16.0 78620 3.250000 145.0 0.0 1553521 16.0 55141 2.000000 95.0 2.0 1553522 16.0 152752 1.083333 177.0 13.0 1553523 16.0 100695 0.000000 137.0 4.0 <p>1553524 rows \u00d7 5 columns</p> <p>From this point on, we will limit our demonstration to just 2 students' data for simplicity. (We picked 2 students, 111292 and 21013, whose data showed that they changed majors at some point.)</p> <p>First, we prepare separate dataframes, one for each feature of the students' data (for example, one TimeSeries object contains the <code>major_gpa</code> data for student 21013).</p> <pre><code>student_111292_mask = some_features_df['anon_id'] == 111292\nstudent_21013_mask = some_features_df['anon_id'] == 21013\n\nmajor_gpa_df_111292 = pd.DataFrame(some_features_df[student_111292_mask]).drop(['cumulative_credit_hours', 'nonmajor_credits', 'anon_id'], axis=1).set_index('semester_idx')\ncredit_hrs_df_111292 = pd.DataFrame(some_features_df[student_111292_mask]).drop(['major_gpa', 'nonmajor_credits', 'anon_id'], axis=1).set_index('semester_idx')\nnonmajor_credits_df_111292 = pd.DataFrame(some_features_df[student_111292_mask]).drop(['cumulative_credit_hours', 'major_gpa', 'anon_id'], axis=1).set_index('semester_idx')\n\nmajor_gpa_df_21013 = pd.DataFrame(some_features_df[student_21013_mask]).drop(['cumulative_credit_hours', 'nonmajor_credits', 'anon_id'], axis=1).set_index('semester_idx')\ncredit_hrs_df_21013 = pd.DataFrame(some_features_df[student_21013_mask]).drop(['major_gpa', 'nonmajor_credits', 'anon_id'], axis=1).set_index('semester_idx')\nnonmajor_credits_df_21013 = pd.DataFrame(some_features_df[student_21013_mask]).drop(['cumulative_credit_hours', 'major_gpa', 'anon_id'], axis=1).set_index('semester_idx')\n\nmajor_gpa_df_111292\n</code></pre> major_gpa semester_idx 1.0 0.7 2.0 0.0 3.0 1.6 4.0 1.5 5.0 0.0 6.0 1.6 7.0 2.4 9.0 2.4 10.0 2.0 <p>Then we create TimeSeries objects from those dataframes:</p> <pre><code>major_gpa_ts_111292 = TimeSeries(major_gpa_df_111292, agg_func=np.mean)\ncredit_hrs_ts_111292 = TimeSeries(credit_hrs_df_111292, agg_func=np.mean)\nnonmajor_credits_ts_111292 = TimeSeries(nonmajor_credits_df_111292, agg_func=np.mean)\n\nmajor_gpa_ts_21013 = TimeSeries(major_gpa_df_21013, agg_func=np.mean)\ncredit_hrs_ts_21013 = TimeSeries(credit_hrs_df_21013, agg_func=np.mean)\nnonmajor_credits_ts_21013 = TimeSeries(nonmajor_credits_df_21013, agg_func=np.mean)\n</code></pre> <p>Now we visualize the data:</p> <pre><code>ax = major_gpa_ts_111292.data.plot(marker='*', label='111292', figsize=(10,4))\nmajor_gpa_ts_21013.data.plot(marker='*', label='21013', figsize=(10,4), ax=ax)\nplt.legend()\nplt.title(\"Major GPA by semester\")\nplt.show()\n\nax = credit_hrs_ts_111292.data.plot(marker='*', label='111292', figsize=(10,4))\ncredit_hrs_ts_21013.data.plot(marker='*', label='21013', figsize=(10,4), ax=ax)\nplt.legend()\nplt.title(\"Cumulative credit hours by semester\")\nplt.show()\n\nax = nonmajor_credits_ts_111292.data.plot(marker='*', label='111292', figsize=(10,4))\nnonmajor_credits_ts_21013.data.plot(marker='*', label='21013', figsize=(10,4), ax=ax)\nplt.legend()\nplt.title(\"Non-major credits by semester\")\nplt.show()\n</code></pre> <p>Now we create a dataframe containing the <code>changed_major</code> values for the 2 students, and then use that dataframe to create an EventSeries object for each student.</p> <pre><code>changed_major_df = student_df[['semester_idx', 'changed_major', 'anon_id']]\nstudent_mask = (changed_major_df['anon_id'] == 21013) | (changed_major_df['anon_id'] == 111292)\nchanged_major_df = pd.DataFrame(changed_major_df[student_mask]).set_index('anon_id')\nchanged_major_df\n</code></pre> semester_idx changed_major anon_id 21013 1.0 0.0 21013 2.0 1.0 21013 3.0 0.0 21013 4.0 0.0 21013 5.0 0.0 21013 6.0 0.0 21013 7.0 0.0 21013 8.0 1.0 21013 9.0 0.0 21013 11.0 0.0 21013 12.0 0.0 21013 13.0 0.0 21013 14.0 0.0 111292 1.0 0.0 111292 2.0 1.0 111292 3.0 0.0 111292 4.0 0.0 111292 5.0 0.0 111292 6.0 0.0 111292 7.0 0.0 111292 9.0 0.0 111292 10.0 0.0 <pre><code>student_21013_mask = changed_major_df.index == 21013\nchanged_major_21013_df = changed_major_df[student_21013_mask]\nchanged_major_21013_df.index = changed_major_21013_df['semester_idx']\nchanged_major_mask = changed_major_21013_df['changed_major'] == 1\nchanged_major_21013_df = pd.DataFrame(changed_major_21013_df[changed_major_mask])\nchanged_major_21013_df = changed_major_21013_df.drop('semester_idx', axis=1)\n\nprint(changed_major_21013_df)\n\nstudent_21013_es = EventSeries(changed_major_21013_df)\n</code></pre> <pre>\n<code>              changed_major\nsemester_idx               \n2.0                     1.0\n8.0                     1.0\n</code>\n</pre> <pre><code>student_111292_mask = changed_major_df.index == 111292\nchanged_major_111292_df = changed_major_df[student_111292_mask]\nchanged_major_111292_df.index = changed_major_111292_df['semester_idx']\nchanged_major_mask = changed_major_111292_df['changed_major'] == 1\nchanged_major_111292_df = pd.DataFrame(changed_major_111292_df[changed_major_mask])\nchanged_major_111292_df = changed_major_111292_df.drop('semester_idx', axis=1)\n\nprint(changed_major_111292_df)\n\nstudent_111292_es = EventSeries(changed_major_111292_df)\n</code></pre> <pre>\n<code>              changed_major\nsemester_idx               \n2.0                     1.0\n</code>\n</pre> <p>We can see that each of these students changed majors in semester 2, and student 21013 changed majors again in semester 8, so we would want to look at trends in features like <code>nonmajor_credits</code> and <code>cumulative_credit_hours</code> leading up to the semesters when the students changed major.</p> <p>We now create 2 separate TimeFrame objects for student 21013 and student 111292 to demonstrate how to use the TimeFrame window function to window the <code>nonmajor_credits</code> and <code>cumulative_credit_hours</code> feature data according to the major change event timestamps.</p> <pre><code>student_21013_tf = TimeFrame()\nstudent_21013_tf['student_21013_es'] = student_21013_es\n# student_21013_df = pd.DataFrame(some_students_df[some_students_df.index == 21013]).drop(['cumulative_credit_hours', 'nonmajor_credits'], axis=1).set_index('semester_idx')\n# print(student_21013_df)\nstudent_21013_tf['student_21013_major_gpa'] = major_gpa_ts_21013\nstudent_21013_tf['student_21013_credit_hrs'] = credit_hrs_ts_21013\nstudent_21013_tf['student_21013_nonmajor_credits'] = nonmajor_credits_ts_21013\n</code></pre> <pre><code>student_111292_tf = TimeFrame()\nstudent_111292_tf['student_111292_es'] = student_111292_es\n# student_111292_df = pd.DataFrame(some_students_df[some_students_df.index == 111292]).drop(['cumulative_credit_hours', 'nonmajor_credits'], axis=1).set_index('semester_idx')\n# print(student_111292_df)\nstudent_111292_tf['student_111292_major_gpa'] = major_gpa_ts_111292\nstudent_111292_tf['student_111292_credit_hrs'] = credit_hrs_ts_111292\nstudent_111292_tf['student_111292_nonmajor_credits'] = nonmajor_credits_ts_111292\n</code></pre> <p>Now we can actually window the time series data by the event timestamps using the <code>TimeFrame</code> window function:</p> <pre><code>student_21013_tf.window(freq=student_21013_es)\n</code></pre> changed_major major_gpa cumulative_credit_hours nonmajor_credits 2.0 1.0 0.000000 22.000000 14.5 8.0 1.0 0.138889 69.166667 10.0 <pre><code>student_111292_tf.window(freq=student_111292_es)\n</code></pre> changed_major major_gpa cumulative_credit_hours nonmajor_credits 2.0 1.0 0.35 22.5 13.0"},{"location":"student-example/#example-applying-tigertail-to-anonymous-student-data","title":"Example: Applying TigerTail to Anonymous Student Data","text":"<p>In this notebook, we apply the TigerTail library to a dataset of anonymous student data downloaded from https://zenodo.org/records/4114005. Each row of this dataset contains information for a certain student (specified by the <code>anon_id</code> feature) at a certain semester (specified by the <code>semester_idx</code> feature). The student information includes static and dynamic features such as <code>female</code>, <code>changed_major</code>, and <code>major_gpa</code>.</p> <p>The eventual goal is to predict the <code>changed_major</code> feature (when a student changes majors) using the other features. This notebook shows how to use TigerTail classes to help visualize patterns in data when a student changes their major.</p>"}]}